{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "atari_games.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/atari_games.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2",
        "colab_type": "text"
      },
      "source": [
        "# Stable Baselines3 - Train on Atari Games\n",
        "\n",
        "Github Repo: [https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)\n",
        "\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines3.\n",
        "\n",
        "It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n",
        "\n",
        "Documentation is available online: [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)\n",
        "\n",
        "## Install Dependencies and Stable Baselines Using Pip\n",
        "\n",
        "\n",
        "```\n",
        "pip install stable-baselines3[extra]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWskDE2c9WoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install stable-baselines3[extra]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm",
        "colab_type": "text"
      },
      "source": [
        "## Import policy, RL agent, ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIedd7Pz9sOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.cmd_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5WNF6G5gWZ1",
        "colab_type": "text"
      },
      "source": [
        "## Training on Atari\n",
        "\n",
        "We will use atari wrapper (it will downsample the image and convert it to gray scale).\n",
        "\n",
        "About Atari preprocessing: [Frame Skipping and Pre-Processing for Deep Q-Networks on Atari 2600 Games](https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/)\n",
        "\n",
        "![Pong](https://cdn-images-1.medium.com/max/800/1*UHYJE7lF8IDZS_U5SsAFUQ.gif)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgjfyOTPVxG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# There already exists an environment generator that will make and wrap atari environments correctly.\n",
        "env = make_atari_env('PongNoFrameskip-v4', n_envs=4, seed=0)\n",
        "# Stack 4 frames\n",
        "env = VecFrameStack(env, n_stack=4)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3K4rMXwimBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = A2C('CnnPolicy', env, verbose=1)\n",
        "model.learn(total_timesteps=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeHL17fViaF0",
        "colab_type": "text"
      },
      "source": [
        "## Download / Upload Trained Agent and Continue Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R-QNCA2KfnX",
        "colab_type": "text"
      },
      "source": [
        "Save and download trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxzQrCqUEO-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkWsoZ8emt0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"a2c_pong\")\n",
        "files.download(\"a2c_pong.zip\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWl9nADggPdY",
        "colab_type": "text"
      },
      "source": [
        "Upload train agent from your local machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZPwefW0J-ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtIidKEihDJ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "20700bdc-0315-42b9-86e9-1538e8ed6832"
      },
      "source": [
        "!du -h a2c*"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.0M\ta2c_pong.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xce8nOwggTcJ",
        "colab_type": "text"
      },
      "source": [
        "Load the agent, and then you can continue training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HTD3tdtLZLS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eb4a7728-94a3-48f6-cca0-3bb762caee17"
      },
      "source": [
        "trained_model = A2C.load(\"a2c_pong\", verbose=1)\n",
        "env = make_atari_env('PongNoFrameskip-v4', n_envs=4, seed=0)\n",
        "env = VecFrameStack(env, n_stack=4)\n",
        "trained_model.set_env(env)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrapping the env in a VecTransposeImage.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SNEC5lphWZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trained_model.learn(int(0.5e6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOj2PoPuhjOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trained_model.save(\"a2c_pong_2\")\n",
        "files.download(\"a2c_pong_2.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
